---
title: "Electric consumption Forecasting"
author: "Edgar KOUAJIEP"
date: '2022-09-06'
output:
  word_document: default
  pdf_document: default
---

# 1. Working environment and Data preparation
All the packages should be installed before loading them
```{r }

listOfPackages <- c("readxl","xlsx","dplyr","ggplot2","fpp2","forecast","keras","vars")

for (i in listOfPackages){

     if(! i %in% installed.packages()){
       print(paste("Installing the package ",i, " ..."))
       install.packages(i, dependencies = TRUE)
       print(paste("Installation done"))
     }
     else {
       print(paste("The package ", i, " is already installed"))
     }

}
```

Load all the necessary libraries. 
```{r }
rm(list = ls()) # clear the global environment 

library(readxl)
library(xlsx)
library(dplyr)
library(ggplot2)
library(fpp2)
library(stats)
library(forecast)
library(keras)
library(vars)
```

Now, let's load the data and verify if there are non-missing values

```{r }
elec_data <- read_excel('Elec-train.xlsx')
sum(is.na(elec_data$`Power (kW)`))
sum(is.na(elec_data$`Temp (C째)`))
```

The power consumption data is recorded between the 1/1/2010 1:15 to 2/17/2010 23:45. In addition, the outdoor air temperature are available for 2/18/2010 meaning the missing values we counted belong to the 18th February, day for which we must forecast the power consumption.

Thus, we will store the temperature of this time range because they will serve us during the forecasting with covariates. Next, we will remove the NA values because we don't need them for our models training.

```{r }
temp_feb18 <- elec_data[is.na(elec_data$`Power (kW)`),3]
elec_data <- elec_data[!is.na(elec_data$`Power (kW)`),]
tail(elec_data,10)
```

To facilitate the forecasting, we will convert each column into time series. To perform this task, we need to identify at which timestep the 1st recording was done. In fact, the data were recorded every 15mn, implying that a 24h-day is equivalent 96 periods. Plus, we know the 1st data record was done at 01:15 corresponding to 5 \* 15 mn. Hence, it corresponds to 5 periods later to the start of the day (00:00 or c(1,1)), meaning our 1st recording started at c(1,6)

```{r }
tspower <- ts(elec_data[,2], start = c(1,6),frequency=96) 
tstemp <- ts(elec_data[,3], start = c(1,6),frequency=96)
tstemp_feb18 <- ts(temp_feb18, start = c(49,1),frequency=96)
```

# 2. Exploratory Data Analysis (EDA)

Let's plot the power consumption and the outdoor temperature

```{r }
autoplot(tspower, series = "Electricity Consumption", ylab = "Power (kW)")
autoplot(tstemp, series = 'Before the 18th February', ylab = 'Outdoor Temperature (째C)') + 
  autolayer(tstemp_feb18, series = 'After the 18th February')

plot(tstemp, tspower, 
     title = 'Consumption vs Temp', 
     xlab = "Outdoor temperuature",
     ylab = "Power consumption" )
```

They seem to be a cyclic pattern on the power consumption, which is also transcribed into the outdoor air temperature. On top of that, we can notice that :

-   The demand in general increases with the temperature

-   For temperatures above 15째C, the demand is always high, at the opposite of the case when temperatures are below 6째C. Elsewhere, the behavior of the consumption is quite the same, switching between high and low demands.

Let's check any trend, seasonality and residuals

```{r }
ggseasonplot(tspower,polar=TRUE) + theme(legend.position="none")
ggseasonplot(tstemp) + theme(legend.position="none")

tspower %>% ggtsdisplay()
tstemp %>% ggtsdisplay()
```

Both ACF are sinusoidal implying a periodic ACF with peaks recurring every 96 lags. Most of their ACF are out of the bounds of statistical significance, meaning that there is a significant autocorrelation in their residuals

**a) On the power consumption :** There is also a slight decrease of the variance (that can be seen on the residuals with the decrease of their high peaks and the same value for their low peaks). Reading the ACF informs that there is no visible linear trend. Plus, a pure seasonal pattern can be seen and described from the seasonal plot:

-   Between 34th period (8h30) and 93th period (23h), the demand is in general high, reaching and maintaining its highest value after the 70th period (17h30), that is, in the evening

-   During the night, that is, from the 94th period (23h15) to the 33th period (8h15), the demand drops and maintains low.

**b) On the outdoor temperature :** Reading the ACF informs us that there is a upward and linear trend (each year, the temperature becomes hotter at this period). Plus, the seasonal plot shows us a pure seasonal pattern :

-   Between the 1st period (00h) and 24th period (6h), the temperature decreases to its lowest point

-   Between the 25th period (6h15) and 64th period (18h), the temperature increases up to its highest point

-   From there to the 96th period (23h45), the temperature decreases again

**NB:** It should be noted at first sight , the two time series does not follow the same seasonal pattern. Plus, the plot between the consumption and the outdoor temperature shows no specific relationship between the two. It can be inferred that there is an odd that the use of covariates will not significantly improve our forecasting. Moreover, the behavior of the consumption is mostly related to the period of time in the day (mostly working hours and hours when people are at their homes, back from their respective work)

# 3. Evaluation of Forecasting models

Before attempting any forecasting, we need to split the data into training set and test set. The test should span on the same time range as the forecasting range, that is 96 periods

```{r }
tselec_data <- cbind(tspower,tstemp)
colnames(tselec_data) <- c("Power","Temp")

xtrain=head(tselec_data,length(tselec_data[,'Power'])-96)
xtest=tail(tselec_data,96)
```

## 3.1. With Univariate

Let's first of all instantiate the metrics comparison table

```{r }
Metrics <- c('MAPE','AIC','BIC','Parameters count','Runtime (s)')
N = length(Metrics)
perfdf <- data.frame(HW = c(rep(0,N)),
                     AutoAR = c(rep(0,N)),
                     ManualAR = c(rep(0,N)),
                     NNAR = c(rep(0,N)),
                     row.names = Metrics)
```

### 3.1.1. Holt winters
```{r }
# Create the model and the fit
start.time <- Sys.time()
modelhw = HoltWinters(xtrain[,1], seasonal = c("additive", "multiplicative"))
fithw = forecast(modelhw,h=96)
end.time <- Sys.time()

# Plot the true and forecasts
autoplot(xtest[,1], ylab ='Power (kW)', series = 'true data') +
  autolayer(fithw$mean,series ='Holt Winters forecasts', PI = FALSE) 

# Compute and save the metrics
perfdf['MAPE','HW'] <- mean(100*abs((xtest[,1]-fithw$mean)/xtest[,1]))
perfdf['AIC','HW'] <- NA
perfdf['BIC','HW'] <- NA
perfdf['Parameters count','HW'] <- length(modelhw$coef)
perfdf['Runtime (s)','HW'] <- round(as.numeric(difftime(end.time, start.time, units="secs")),2)
```


It shoudl be noted using the *hw()* function instead of the *HoltWinters()* function will result to an error. In fact, the *hw()* function need to estimate the initial states of the seasonal components which are 96 states (implying high frequency due to large period). With this large number of states, we will surely overfit which is anticipated by that function.


### 3.1.2. Auto ARIMA

```{r }
# Create the model and the fit
start.time <- Sys.time()
modelautoARM = auto.arima(xtrain[,1], lambda = 'auto')
fitautoARM = forecast(modelautoARM,h=96)
end.time <- Sys.time()

# Check its summary & residuals
modelautoARM %>% summary()
modelautoARM %>% residuals()%>% ggtsdisplay()

# Plot the true and forecasts
autoplot(xtest[,1], ylab ='Power (kW)', series = 'true data') +
  autolayer(fitautoARM$mean,series ='Auto ARIMA forecasts', PI = FALSE) 

# Compute and save the metrics
perfdf['MAPE','AutoAR'] <- mean(100*abs((xtest[,1]-fitautoARM$mean)/xtest[,1]))
perfdf['AIC','AutoAR'] <- modelautoARM$aic
perfdf['BIC','AutoAR'] <- modelautoARM$bic
perfdf['Parameters count','AutoAR'] <- length(modelautoARM$coef)
perfdf['Runtime (s)','AutoAR'] <- round(as.numeric(difftime(end.time, start.time, units="secs")),2) 
```

Auto ARIMA with Box Cox transformation seems to work pretty well ! However, there are still some correlated residuals according to the ACF and PACF. Let's see if we can manually improve the ARIMA model

### 3.1.3. Manual ARIMA

```{r }
lam = BoxCox.lambda(xtrain[,1]) # Find the best lambda
xtrainBC =  BoxCox(xtrain[,1],lam) # Stabilize any potential variance
xtrainBC %>% diff(lag = 96) %>% ggtsdisplay() 
```

With the seasonality removed, we verify if the residuals is not a white noise

```{r }
Box.test(diff(xtrainBC, lag=96),type="Ljung-Box")
```

Thanks to the p-value being less than 5%, we can reject the null-hypothesis and confirm that the residuals is not a white noise. Next, we can perform its modelling with the ARIMA method.

From the ACF and PACF graph, we can tune :

-   the non-seasonal part as an AR1 model: exponential decrease of the ACF and significant PCF at lag 5.

-   the seasonal part as an MA1 model: exponential decrease in the seasonal lags of the PACF and a significant ACF at lag = 96

```{r }
modelmanualARM = Arima(xtrainBC, order=c(5,0,0),seasonal=c(0,1,1))
summary(modelmanualARM)
modelmanualARM %>% residuals()%>% ggtsdisplay()
```

Even though there are still some autocorrelation coefficients left, we think the tuning is sufficient since those ACF have low absolute values (\<0.05) and we want to avoid a long runtime and overfitting (due to the increase of number of parameters). So we can assume that at this stage of tuning, the residuals are uncorrelated. With the ARIMA model manually tuned, we can then perform some forecasting on the test set

```{r }
# Create the model and the fit
start.time <- Sys.time()
modelmanualARM = Arima(xtrain[,1], lambda = lam, order=c(5,0,0),seasonal=c(0,1,1))
fitmanualARM = forecast(modelmanualARM, h=96)
end.time <- Sys.time()

# Check its summary
modelmanualARM %>% summary()
 
# Plot the true and forecasts
autoplot(xtest[,1],series='true data') +
 autolayer(fitmanualARM$mean,series='Manual ARIMA forecasts',PI=FALSE)

# Compute and save the metrics
perfdf['MAPE','ManualAR'] <- mean(100*abs((xtest[,1]-fitmanualARM$mean)/xtest[,1]))
perfdf['AIC','ManualAR'] <- modelmanualARM$aic
perfdf['BIC','ManualAR'] <- modelmanualARM$bic
perfdf['Parameters count','ManualAR'] <- length(modelmanualARM$coef)
perfdf['Runtime (s)','ManualAR'] <- round(as.numeric(difftime(end.time, start.time, units="secs")),2) 
```

The MAPE is slightly less than the Auto ARIMA's one but the trade off still makes sense since we better the AIC and BIC. Visually, the manual ARIMA forecast seems to follow well the true data but not that well as auto ARIMA, especially on the extreme peaks (between 250 and 350 kW)

### 3.1.4. Neural Networks Auto Regression (NNAR)

```{r }
# Create the model and the fit
start.time <- Sys.time()
modelNNAR = nnetar(xtrain[,1],lambda = lam)
fitNNAR = forecast(modelNNAR, h=96)
end.time <- Sys.time()

# Check its summary
print(modelNNAR)

# Plot the true and forecasts
autoplot(xtest[,1],series='true data') +
 autolayer(fitNNAR$mean,series='NNAR forecasts',PI=FALSE)

# Compute and save the metrics
perfdf['MAPE','NNAR'] <- mean(100*abs((xtest[,1]-fitNNAR$mean)/xtest[,1]))
perfdf['AIC','NNAR'] <- NA
perfdf['BIC','NNAR'] <- NA
perfdf['Parameters count','NNAR'] <- length(modelNNAR$model[[1]]$wts)
perfdf['Runtime (s)','NNAR'] <- round(as.numeric(difftime(end.time, start.time, units="secs")),2) 
```

### 3.1.5. Summary

```{r }
perfdf
```

The best model (that is, with the best compromise) is the auto ARIMA model because it is a good trade off in terms of MAPE and runtime, it has the lowest number of parameters used and it is one that visually fits well the test set.

Let's forecast the power consumption on the 18th February

```{r }
modelautoARM = auto.arima(tselec_data[,1], lambda = 'auto')
bestfit_univariate = forecast(modelautoARM,h=96)$mean
```

## 3.2. With Covariates

Let's first of all instantiate the metrics comparison table

```{r }
perfdf_multi <- data.frame(DRMAutoARM = c(rep(0,N)),
                           DRMManualARM = c(rep(0,N)),
                           NNAR1 = c(rep(0,N)),
                           row.names = Metrics)
```

### 3.2.1. Dynamic Regression Model - Auto ARIMA

```{r }
# Create the model and the fit
start.time <- Sys.time()
modelDRMAutoARM = auto.arima(xtrain[,1],xreg=xtrain[,2],lambda = 'auto')
fitDRMAutoARM = forecast(modelDRMAutoARM,h=96,xreg=xtest[,2])
end.time <- Sys.time()

# Check its summary
modelDRMAutoARM %>% summary()
modelDRMAutoARM %>% residuals()%>% ggtsdisplay()
checkresiduals(modelDRMAutoARM)

# Plot the true and forecasts
autoplot(xtest[,1], ylab ='Power (kW)', series = 'true data') +
  autolayer(fitDRMAutoARM$mean, series ='DRM - Auto ARIMA forecasts', PI = FALSE) 

# Compute and save the metrics
perfdf_multi['MAPE','DRMAutoARM'] <- mean(100*abs((xtest[,1]-fitDRMAutoARM$mean)/xtest[,1]))
perfdf_multi['AIC','DRMAutoARM'] <- modelDRMAutoARM$aic
perfdf_multi['BIC','DRMAutoARM'] <- modelDRMAutoARM$bic
perfdf_multi['Parameters count','DRMAutoARM'] <- length(modelDRMAutoARM$coef)
perfdf_multi['Runtime (s)','DRMAutoARM'] <- round(as.numeric(difftime(end.time, start.time, units="secs")),2) 
```

Similar to its univariate equivalent (auto ARIMA), the forecasts works well even though there is not that much of an improvement (MAPE : 4.47 vs 4.4). Furthermore, if we check the residuals, there are some correlated coefficients. Let's try to improve this

### 3.2.2. Dynamic Regression Model - Auto ARIMA

We start by removing the effect of the trend, the season and the covariate by using TS Linear Model

```{r }
fit1=tslm(Power~Temp+trend+season,data=xtrain)
summary(fit1)
CV(fit1)
```

the R-squared and its adjusted seems pretty good. All the features seem significant. However, the correlation coefficient for the trend is close to 0 even if its associated p-value is relevant. We can note seasonal terms after the 26th one are the only relevant based on their p-value. Let's check for the residuals

```{r }
checkresiduals(fit1)
fit1$residuals %>% ggtsdisplay()
```

We can see that there is an exponential decrease of the ACF and an important PAC at lag = 5. Maybe AR5 will model this residuals

```{r }
tmp=fit1$residuals
fit2=Arima(tmp,order=c(5,0,0))
checkresiduals(fit2)
```

Even though there are some terms that are out of the bounds of statistical significance, their values are really low. Furthermore, adding more coefficients will just increase the chance of overfitting. So we can consider at this stage of tuning that the corresponding residuals are uncorrelated.

In conclusion, We can propose an SARIMA model with non seasonal part equal to c(5,0,0) and seasonal part equal to c(0,1,0) (to take into account season & trend):

```{r }
fit=Arima(xtrain[,"Power"],xreg=xtrain[,"Temp"],order=c(5,0,0),seasonal = c(0,1,0))
fit$residuals %>% ggtsdisplay()
```

We set now the seasonal part to c(0,1,1) to take into account the exponential decrease on the seasonal lags (96, 192, 288) on PACF and the spike at lag = 96 on the ACF

```{r }
fit=Arima(xtrain[,"Power"],xreg=xtrain[,"Temp"],order=c(5,0,0),seasonal = c(0,1,1))
fit$residuals %>% ggtsdisplay()
```

As seen earlier, the tuning is enough as we wish to avoid overfitting and long runtime. Now we can perform forecast and compute metrics

```{r }
# Create the model and the fit
start.time <- Sys.time()
modelDRMManualARM = Arima(xtrain[,"Power"],xreg=xtrain[,"Temp"],order=c(5,0,0),seasonal = c(0,1,1), lambda = lam)
fitDRMManualARM = forecast(modelDRMManualARM, h=96,xreg=xtest[,2])
end.time <- Sys.time()
 
# Plot the true and forecasts
autoplot(xtest[,1],series='true data') +
 autolayer(fitDRMManualARM$mean,series='DRM - Manual ARIMA forecasts',PI=FALSE)

# Compute and save the metrics
perfdf_multi['MAPE','DRMManualARM'] <- mean(100*abs((xtest[,1]-fitDRMManualARM$mean)/xtest[,1]))
perfdf_multi['AIC','DRMManualARM'] <- modelDRMManualARM$aic
perfdf_multi['BIC','DRMManualARM'] <- modelDRMManualARM$bic
perfdf_multi['Parameters count','DRMManualARM'] <- length(modelDRMManualARM$coef)
perfdf_multi['Runtime (s)','DRMManualARM'] <- round(as.numeric(difftime(end.time, start.time, units="secs")),2) 
```

### 3.2.3. Neural Networks AutoRegression

```{r}
# Create the model and the fit
start.time <- Sys.time()
modelNNAR1 = nnetar(xtrain[,1],xreg=xtrain[,2],lambda = lam)
fitNNAR1 = forecast(modelNNAR1, h=96,xreg=xtest[,2])
end.time <- Sys.time()

# Check its summary
print(modelNNAR1)

# Plot the true and forecasts
autoplot(xtest[,1],series='true data') +
 autolayer(fitNNAR1$mean,series='NNAR forecasts',PI=FALSE)

# Compute and save the metrics
perfdf_multi['MAPE','NNAR1'] <- mean(100*abs((xtest[,1]-fitNNAR1$mean)/xtest[,1]))
perfdf_multi['AIC','NNAR1'] <- NA
perfdf_multi['BIC','NNAR1'] <- NA
perfdf_multi['Parameters count','NNAR1'] <- length(modelNNAR1$model[[1]]$wts)
perfdf_multi['Runtime (s)','NNAR1'] <- round(as.numeric(difftime(end.time, start.time, units="secs")),2) 
```

### 3.2.4. Summary

```{r}
perfdf_multi
```

In terms of AIC/BIC, the DRM - Manual ARM seems the best model. However, we think the best model should be a good compromise with other parameters. That's why we choose the auto ARIMA model. Precisely, it is a good trade off in terms of MAPE, parameters count and runtime. Plus,it is the best that visually fits the test set.

```{r }
modelDRMAutoARM = auto.arima(tselec_data[,1],xreg=tselec_data[,2],lambda = 'auto')
bestfit_covariate = forecast(modelDRMAutoARM, h=96, xreg = tstemp_feb18)$mean
```
#4. Conclusions & Discussions
Let's store the best fits in an excel file

```{r }
# Create the bestfit vector
bestfit = cbind(bestfit_univariate, bestfit_covariate)
colnames(bestfit) = cbind('forecast with univariate','forecast with covariate')

# Create a convenient file name yyyy_MM_dd_hh_mm_ss in order to be unique at each simulation
outputfilename = Sys.time() 

outputfilename <- outputfilename %>% {gsub('[\t\n]', '',.)} %>% {gsub('[ :-]', '_',.)} 

outputfilename <- paste(outputfilename,'.xlsx',sep="")

# Save the results in an Excel File
write.xlsx(bestfit,outputfilename, sheetName = "Sheet1", col.names = TRUE, append = TRUE)
```
 
Let's compare models with univariate and models with covariate
``` {r }
autoplot(bestfit)
```

```{r }
cbind(perfdf,perfdf_multi)
```

Performance-wise, using covariates doesn't help the forecasting techniques that much. This was anticipated during the EDA. For some models like NNAR, it worsens it (MAPE, runtime and number of parameters increase).
